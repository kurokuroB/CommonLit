{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "#引数は、optimizer,学習率を上げるステップ数、トータルステップ数。\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import gc\n",
    "\n",
    "#自動ガベージコレクションを有効にするらしい。\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "MAX_LEN = 300\n",
    "#eval頻度を調整→eval速度と、最適なmodel抽出のバランス設定。\n",
    "EVAL_SCHEDULE = [(0.465,16),(-1., 8)]\n",
    "ROBERTA_PATH = \"../input/clrp-roberta-large/clrp_roberta_large\"\n",
    "TOKENIZER_PATH = \"../input/clrp-roberta-large/clrp_roberta_large\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "num_bins = int(np.floor(1 + np.log2(len(train_df))))\n",
    "train_df['bins'],bins=pd.cut(train_df['target'],num_bins,labels=False,retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df= pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove incomplete entries if any→スコアと標準偏差が０のデータを削除\n",
    "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
    "              inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "\n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)   \n",
    "            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            standard_error=self.standard_error[index]\n",
    "            return (input_ids, attention_mask, target,standard_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        #https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig　参照\n",
    "        config.update({\"output_hidden_states\":True, #Whether or not the model should return all hidden-states.\n",
    "                       \"hidden_dropout_prob\": 0.0,# The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "                       \"layer_norm_eps\": 1e-7})                     \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "          \n",
    "        #sequence内の各ワードを重み付けしている。\n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 700),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(700, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor1 = nn.Sequential(                        \n",
    "            nn.Linear(1024, 1)                        \n",
    "        )\n",
    "        \n",
    "        self.regressor2 = nn.Sequential(                        \n",
    "            nn.Linear(1024, 1)                        \n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_layer_hidden_states.size())\n",
    "\n",
    "        attention_last_layer=last_layer_hidden_states*input_mask_expanded\n",
    "\n",
    "        weights = self.attention(attention_last_layer)\n",
    "                \n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # regressor2で、standard_errorの予測値を出力\n",
    "        return self.regressor1(context_vector),self.regressor2(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()            \n",
    "    target_mse = 0\n",
    "    std_mse=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask, target,standard_error) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)                        \n",
    "            target = target.to(DEVICE)           \n",
    "            standard_error=standard_error.to(DEVICE)  \n",
    "            \n",
    "            target_pred,std_pred = model(input_ids, attention_mask)                       \n",
    "\n",
    "            target_mse += nn.MSELoss(reduction=\"sum\")(target_pred.flatten(), target).item()\n",
    "            \n",
    "            #一応、std_mseもモニタリング\n",
    "            std_mse += nn.MSELoss(reduction=\"sum\")(std_pred.flatten(), standard_error).item()\n",
    "                \n",
    "\n",
    "    return target_mse / len(data_loader.dataset),std_mse / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))#numpy配列にすることで、計算が楽になる。   \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            \n",
    "            #std_predは使わない\n",
    "            target_pred,_= model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + target_pred.shape[0]] = target_pred.flatten().to(\"cpu\")#バッチごとに出力\n",
    "            index += target_pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_path, train_loader, val_loader,\n",
    "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS,alpha=1):    \n",
    "    best_val_rmse = None\n",
    "    best_epoch = 0\n",
    "    step = 0\n",
    "    last_eval_step = 0\n",
    "    eval_period = EVAL_SCHEDULE[0][1]#上部に定義したタプルの右側   \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):                           \n",
    "        val_rmse = None         \n",
    "\n",
    "        for batch_num, (input_ids, attention_mask, target,standard_error) in enumerate(train_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)            \n",
    "            target = target.to(DEVICE)\n",
    "            standard_error= standard_error.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            target_pred,std_pred= model(input_ids, attention_mask)\n",
    "            \n",
    "            #reductionはsumにし、合算してから、平均を算出\n",
    "            target_mse = nn.MSELoss(reduction=\"sum\")(target_pred.flatten(), target)\n",
    "            std_mse = nn.MSELoss(reduction=\"sum\")(std_pred.flatten(), standard_error)\n",
    "                        \n",
    "            sum_mse=(target_mse + alpha*std_mse)/len(input_ids)\n",
    "            \n",
    "            sum_mse.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            #EVAL_SCHEDULE[0][1]ごとにeval実行\n",
    "            if step >= last_eval_step + eval_period:\n",
    "                # Evaluate the model on val_loader.\n",
    "                elapsed_seconds = time.time() - start\n",
    "                num_steps = step - last_eval_step\n",
    "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                last_eval_step = step\n",
    "                \n",
    "                \n",
    "                val_target_mse,val_std_mse=eval_mse(model, val_loader)\n",
    "                val_rmse = math.sqrt(val_target_mse)                            \n",
    "\n",
    "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
    "                      f\"val_rmse: {val_rmse:0.4}\\n std_error_val_mse:{val_std_mse}\")\n",
    "                \n",
    "                #一定のrmseを超えると、evalの頻度が上がる。\n",
    "                for rmse, period in EVAL_SCHEDULE:\n",
    "                    if val_rmse >= rmse:\n",
    "                        eval_period = period\n",
    "                        break                               \n",
    "                #best_valが０または、val_rmseがベストスコアになった時はbestスコアを更新\n",
    "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "                else:       \n",
    "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "                          f\"(from epoch {best_epoch})\")                                    \n",
    "                    \n",
    "                start = time.time()\n",
    "                                            \n",
    "            step += 1\n",
    "                        \n",
    "    \n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    \n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:-8]    \n",
    "    attention_parameters = named_parameters[-8:-4]\n",
    "    regressor1_parameters = named_parameters[-4:-2]\n",
    "    regressor2_parameters = named_parameters[-2:]\n",
    "\n",
    "    parameters = []\n",
    "    lr = 2e-5\n",
    "    weight_decay=1e-2\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    " #       weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        \n",
    "\n",
    "#         if layer_num >= 69:        \n",
    "#             lr = 5e-5\n",
    "\n",
    "#         if layer_num >= 133:\n",
    "#             lr = 1e-4\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"lr\": lr})\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor1_group = [params for (name, params) in regressor1_parameters]\n",
    "    regressor2_group = [params for (name, params) in regressor2_parameters]\n",
    "    \n",
    "    parameters.append({\"params\": attention_group,\"lr\": lr,\"weight_decay\": weight_decay})\n",
    "    parameters.append({\"params\": regressor1_group,\"lr\": lr,\"weight_decay\": weight_decay})\n",
    "    parameters.append({\"params\": regressor2_group,\"lr\": lr,\"weight_decay\": weight_decay})\n",
    "\n",
    "\n",
    "\n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "SEED = 150\n",
    "#各Foldごとのベストスコアを格納\n",
    "list_val_rmse = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df,train_df['bins'])):    \n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    #foldごとにベストスコアのパラメータを保存するイメージ。\n",
    "    model_path = f\"model_{fold + 1}.pth\"\n",
    "        \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=2)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=2)    \n",
    "        \n",
    "    #set_random_seed(SEED + fold)    \n",
    "    \n",
    "    model = LitModel().to(DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model)                          \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
    "        num_warmup_steps=50)    \n",
    "    \n",
    "    list_val_rmse.append(train(model, model_path, train_loader,\n",
    "                               val_loader, optimizer, scheduler=scheduler,alpha=1))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nPerformance estimates:\")\n",
    "    print(list_val_rmse)\n",
    "    print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Foldごとのpredictionを求める。\n",
    "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for index in range(len(list_val_rmse)):            \n",
    "    model_path = f\"model_{index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path))#model_pathを解凍。→state_dictを読み込み。   \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_predictions[index] = predict(model, test_loader)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = all_predictions.mean(axis=0)\n",
    "submission_df.target = predictions\n",
    "#print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
